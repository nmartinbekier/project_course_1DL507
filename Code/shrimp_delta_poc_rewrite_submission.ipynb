{"cells":[{"cell_type":"markdown","source":["### Python wheel created from trase utility code"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"781f4b5a-29c2-4c04-b79c-154ec5fb29ff","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%pip install /dbfs/FileStore/jars/f90d9f8d_fd2a_4845_b5b0_cdfd9213b291/trase_tools-0.1.0-py3-none-any.whl"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"488751f2-0f9c-472b-af75-969fb49ee392","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### import works"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a159c3c8-92db-4be4-81c5-cf73d1bb210b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import trase_tools\nfrom trase_tools.tools.utilities.helpers import *\ni = Identifier(\"Cote D'Ivoire\")\nprint(i.upper_space)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"77edb67a-aad2-4400-9e95-735089023756","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Single-user check failed: user 'nicolas.martin.bekier@gmail.com' attempted to run a command on single-user cluster 1116-120005-ljbdje2v, but the single user of this cluster is 'raazesh.sainudiin.math.uu.se.dbuaEU@gmail.com'","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import dlt\nimport io\nimport numpy as np\n\nfrom pyspark.sql.functions import *\nimport pyspark.pandas as ps\nimport pandas as pd\nspark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n#spark.conf.set(\"spark.databricks.delta.schema.overwriteSchema.enabled\",\"true\")\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f4e66c77-505b-45cc-8e10-9f9e7076655b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Functions that replace the load and write helper functions from original Trase code. \n__The aim is to generalize the helper functions with delta code, for seamless integration of Delta Live Tables__"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9367fc21-2569-4f9d-9e22-f413c64a1953","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["\ndef get_df_clean(key,sep=\";\",encoding=\"latin1\",bucket_name=\"s3a://uutrase/data/trase-uppsala/\",type=None,skiprows=None):\n    \n    return spark.createDataFrame(remove_whitespace(spark.read.option(\"delimiter\", sep)\n                             .option(\"encoding\",\"UTF-8\")\n                             .option(\"header\", \"true\")\n                             .option(\"multiline\",\"true\")\n                             .csv(bucket_name + key).toPandas()))\n    \n#get data from existing table\n\n    \n    \ndef remove_whitespace(df):\n    for col in df.columns:\n        if df[col].dtype in (np.object, np.str):\n            df[col] = df[col].str.strip()\n            \n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ba2ce4d1-92a1-4c05-ae68-92d978ae5e92","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndef get_df_while_creating_table(key,sep=\";\",encoding=\"latin1\",bucket_name=\"s3a://uutrase/data/trase-uppsala/\",type=None,skiprows=None):\n        table_name = key.split(\"/\")[-1].split(\".\")[0].replace(\"-\",\"_\")\n        df = get_df_clean(key,sep,encoding,bucket_name,skiprows)\n        spark_df = spark.createDataFrame(df)\n        @dlt.create_table(name = table_name)\n        def create_trase_table():\n            return spark_df.select([col(c).alias(c.replace(\" \", \"_\")) for c in spark_df.columns]) \n        return df\n            \n        \ndef get_df(table_name):\n#    return ps.DataFrame(dlt.read(\"city_df\").select(\"*\")).to_pandas()\n    return ps.DataFrame(dlt.read(table_name)).to_pandas()\n\ndef delta_to_pandas(table_name):\n    py_pandas_df = pd.DataFrame(dlt.read(table_name))\n    print(\"py_pandas_df\",py_pandas_df)\n    return py_pandas_df\n\ndef zero_round():\n    @dlt.table(name=\"first_table\")\n    def df_table():\n        pandas_df = delta_to_pandas(\"city_df\")\n\n        print(\"regular pandas: \",pandas_df)\n        \n        return spark.createDataFrame(pandas_df)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5481a9f2-209d-48e2-803e-fa4d31625729","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Trase script, slightly modified"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"61ba9e50-bc09-495e-bf1e-707a92e3f10b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def shrimp_processing():\n    city_df = get_df(\"city_df\")\n    parish_df = get_df(\"parish_df\")\n    print(\"parish_df\",len(parish_df))\n    city_df = pd.merge(\n        city_df,\n        parish_df,\n        how=\"left\",\n        left_on=[\"PARISH\", \"CANTON\"],\n        right_on=[\"PARISH_NAME\", \"CANTON_NAME\"],\n    )\n    print(\"merged city_df\",len(city_df))\n    city_df[\"PARISH_TRASE_ID\"] = city_df[\"TRASE_ID\"]\n    city_df = city_df[[\"PARISH_TRASE_ID\", \"CITY\", \"REGION\", \"CANTON\"]]\n\n\n    eu_df = get_df(\"eu_df\")\n    print(\"eu_df\",len(eu_df))\n    eu_codes = eu_df.CODE.tolist()\n\n    china_df = get_df(\"china_df\")\n    china_df[\"CODE\"] = china_df[\"APPROVALNO_CLEAN\"]\n    print(\"china_df\",len(china_df))\n    china_codes = china_df.CODE.tolist()\n\n    df1 = get_df(\"df1\")\n    df2 = get_df(\"df2\")\n    df3 = get_df(\"df3\")\n\n    df3 = df3[\n        [\"CODIGO\", \"NOMBRE\", \"DIRECCION_ESTABLECIMIENTO\", \"TELEFONOS\", \"CONTACTO\"]\n    ].rename(columns={\"CODIGO\": \"CODIGO\"})\n    # Remove duplicates in joint dataset - dfpesca2017.nunique()\n    print(\"df1\",len(df1))\n    print(\"df2\",len(df2))\n    print(\"df3\",len(df3))\n    dfpesca2017 = pd.concat([df1, df2])\n    dfpesca2017.drop_duplicates(\n        subset=[\"CODIGO\", \"NOMBRE\", \"DIRECCION_ESTABLECIMIENTO\", \"CONTACTO\"], inplace=True\n    )\n    # dfpesca2017 = dfpesca2017[[]]\n    df2017 = pd.concat([dfpesca2017, df3])\n    print(len(df2017[\"CODIGO\"].unique()))\n    print(len(df2017))\n\n    print(len(df2017[\"CODIGO\"].unique()))\n    print(len(df2017))\n\n\n    assert len(df2017[\"CODIGO\"].unique()) == len(df2017)\n    for column in [\"NOMBRE\", \"DIRECCION_ESTABLECIMIENTO\", \"TELEFONOS\", \"CONTACTO\"]:\n        df2017[column] = df2017[column].str.replace(r\"\\r\", \" \")\n        df2017[column] = df2017[column].str.replace(r\"\\n\", \" \")\n\n    df1_2018 = get_df(\"df1_2018\")\n    df2_2018 = get_df(\"df2_2018\")\n    df2018 = pd.concat([df1_2018, df2_2018])\n    assert len(df2018[\"CODIGO\"].unique()) == len(df2018)\n    for column in [\"NOMBRE\", \"DIRECCION_ESTABLECIMIENTO\", \"TELEFONOS\", \"CONTACTO\"]:\n        #print(df2018.columns)\n        df2018[column] = df2018[column].str.replace(r\"\\r\", \" \")\n        df2018[column] = df2018[column].str.replace(r\"\\n\", \" \")\n\n    dfs = []\n    for year, df in {2017: df2017, 2018: df2018}.items():\n        df.rename(\n            columns={\n                \"CODIGO\": \"CODE\",\n                \"NOMBRE\": \"NAME\",\n                \"DIRECCION_ESTABLECIMIENTO\": \"ADDRESS\",\n                \"DIRECCION_ESTABLECIMIENTO\": \"ADDRESS\",\n                \"TELEFONOS\": \"TELEPHONE\",\n                \"CONTACTO\": \"CONTACT\",\n            },\n            inplace=True,\n        )\n\n        # Compare with EU facilities\n        all_codes = df.CODE.tolist()\n        for eu_code in eu_codes:\n            if eu_code not in all_codes:\n                if f\"PPA-{eu_code}\" not in all_codes:\n                    if f\"PA-{eu_code}\" not in all_codes:\n                        print(eu_code)\n                        # TODO: Half of them are 2019, we need to remove these facilities\n                        print(\n                            f\"eu facility: {eu_code}, {eu_df[eu_df.CODE == eu_code].YEAR} \"\n                        )\n\n        # Match with EU dataset by code\n        df[\"SIMPLE_CODE\"] = df.CODE.apply(lambda value: value.split(\"-\")[-1]).astype(int)\n        eu_df[\"SIMPLE_CODE\"] = eu_df.CODE.apply(lambda value: value.split(\"-\")[-1]).astype(\n            int\n        )\n        tdf = pd.merge(df, eu_df, how=\"left\", left_on=\"SIMPLE_CODE\", right_on=\"SIMPLE_CODE\")\n\n        # If there is a match in eu dataset flag this in a new column\n        tdf[\"EU_CERTIFIED\"] = np.where(tdf.CODE_y.isna(), \"FALSE\", \"TRUE\")\n        tdf[\"TYPE\"] = np.where(tdf.CODE_x.str.startswith(\"PPA-\"), \"F Aq\", \"Aq\")\n        tdf.rename(\n            columns={\n                \"CODE_x\": \"CODE\",\n                \"NAME_x\": \"NAME\",\n                \"NAME_y\": \"EU_NAME_SYNONYM\",\n                \"COMMENT\": \"EU_TYPE\",\n            },\n            inplace=True,\n        )\n        tdf = tdf[\n            [\n                \"CODE\",\n                \"NAME\",\n                \"ADDRESS\",\n                \"TELEPHONE\",\n                \"CONTACT\",\n                \"SIMPLE_CODE\",\n                \"CITY\",\n                \"REGION\",\n                \"EU_NAME_SYNONYM\",\n                \"EU_TYPE\",\n                \"EU_CERTIFIED\",\n                \"YEAR\",\n                \"TYPE\",\n            ]\n        ]\n        for china_code in china_codes:\n            if china_code not in all_codes:\n                if f\"PPA-{china_code}\" not in all_codes:\n                    if f\"PA-{china_code}\" not in all_codes:\n                        print(f\"chinese facility: {china_code}\")\n\n        # Match with EU dataset by code\n        #tdf[\"SIMPLE_CODE\"] = tdf[\"SIMPLE_CODE\"].astype(\"int64\")\n        china_df[\"CODE\"] = china_df[\"CODE\"].astype(int)\n        tdf = pd.merge(tdf, china_df, how=\"left\", left_on=\"SIMPLE_CODE\", right_on=\"CODE\")\n        # If there is a match in china dataset flag this in a new column\n        tdf[\"CHINA_CERTIFIED\"] = np.where(tdf.CODE_y.isna(), \"FALSE\", \"TRUE\")\n        # Update\n        tdf[\"CITY\"] = np.where(tdf.CITY.isna(), tdf[\"City_County\"].str.upper(), tdf.CITY)\n        type_dict = {\n            \"A\": \"Aq\",\n            \"A - FISHERY PRODUCTS\": \"F Aq\",\n            \"FISHERY PRODUCTS\": \"F\",\n            \"AQUACULTURE PRODUCTS/FISHERY PRODUCTS\": \"F Aq\",\n            \"AQUACULTURE PRODUCTS\": \"Aq\",\n            \"A - AQUACULTURE PRODUCTS\": \"Aq\",\n        }\n        tdf.Remark = tdf.Remark.map(type_dict)\n        tdf[\"TYPE\"] = np.where(tdf.TYPE.isna(), tdf.Remark, tdf.TYPE)\n        tdf.rename(\n            columns={\"CODE_x\": \"CODE\", \"NAME_x\": \"NAME\", \"NAME_y\": \"CHINA_NAME_SYNONYM\"},\n            inplace=True,\n        )\n        tdf = tdf[\n            [\n                \"CODE\",\n                \"NAME\",\n                \"ADDRESS\",\n                \"TELEPHONE\",\n                \"CONTACT\",\n                \"SIMPLE_CODE\",\n                \"CITY\",\n                \"REGION\",\n                \"EU_NAME_SYNONYM\",\n                \"EU_TYPE\",\n                \"EU_CERTIFIED\",\n                \"CHINA_NAME_SYNONYM\",\n                \"CHINA_CERTIFIED\",\n                \"YEAR\",\n                \"TYPE\",\n            ]\n        ]\n\n        # TODO extract names that do not match : SYNONYMS\n        # names = tdf[tdf.NAME_x != tdf.NAME_y]\n        # Look at non_matches and merge in cantons from city dictionary\n        tdf = pd.merge(tdf, city_df, how=\"left\", left_on=\"CITY\", right_on=\"CITY\")\n\n        # Rename columns and rejigg\n        tdf.rename(\n            columns={\n                \"CODE_x\": \"CODE\",\n                \"NAME_x\": \"NAME\",\n                \"NAME_y\": \"CHINA_NAME_SYNONYM\",\n                \"REGION_x\": \"REGION\",\n            },\n            inplace=True,\n        )\n        tdf = tdf[\n            [\n                \"CODE\",\n                \"NAME\",\n                \"ADDRESS\",\n                \"TELEPHONE\",\n                \"CONTACT\",\n                \"SIMPLE_CODE\",\n                \"CITY\",\n                \"PARISH_TRASE_ID\",\n                \"REGION\",\n                \"CANTON\",\n                \"EU_NAME_SYNONYM\",\n                \"EU_TYPE\",\n                \"EU_CERTIFIED\",\n                \"CHINA_NAME_SYNONYM\",\n                \"CHINA_CERTIFIED\",\n                \"TYPE\",\n            ]\n        ]\n\n        # Final push\n        print(\n            f\"There are {len(tdf[tdf.CANTON.isna()])} addresses to check out of {len(tdf)}.\"\n        )\n        dfs.append(tdf)\n\n    # Combine the two files\n    concat_df = pd.concat(dfs)\n    print(\"concat_df: \",len(concat_df))\n    final_df = concat_df.drop_duplicates(\n        subset=[\n            \"CODE\",\n            \"NAME\",\n            \"SIMPLE_CODE\",\n            \"CITY\",\n            \"PARISH_TRASE_ID\",\n            \"REGION\",\n            \"CANTON\",\n            \"EU_NAME_SYNONYM\",\n            \"EU_TYPE\",\n            \"EU_CERTIFIED\",\n            \"CHINA_CERTIFIED\",\n            \"TYPE\",\n        ]\n    ).reset_index(drop=True)\n    #print(len(final_df))\n    #print(len(final_df.CODE.unique()))\n\n\n\n    assert len(final_df) == len(final_df.CODE.unique())\n\n    # Add source column\n    final_df[\"SOURCE\"] = \"www.acuaculturaypesca.gob.ec\"\n    \n    print(\"final_df: \",final_df)\n    return ps.DataFrame(final_df)\n\ndef create_and_read_table():\n    @dlt.table(name=\"best_table\")\n    def read_table():\n        a = dlt.read(\"city_df\")\n        return a"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a4f3b87-73b8-41f4-a8bb-d497c31a5f34","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Creation of Delta Live Tables"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"72ead25d-b317-481c-989f-61f019f6a966","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["@dlt.table(name=\"city_df\")\ndef get_city_df():\n    return get_df_clean(\"ecuador/spatial/BOUNDARIES_temp/city_to_parish_2.csv\", sep=\",\", encoding=\"utf8\")\n\n@dlt.table(name=\"china_df\")\ndef get_china_df():\n    return get_df_clean(\"ecuador/logistics/shrimp_processing_facilities/out/china/CHINA_matches.csv\",sep=\",\",encoding=\"utf8\")\n\n@dlt.table(name=\"parish_df\")\ndef get_parish_df():\n    return get_df_clean(\"ecuador/spatial/BOUNDARIES_out/ec_parishes.csv\", sep=\";\", encoding=\"utf8\")\n\n\n@dlt.table(name=\"eu_df\")\ndef get_eu_df():\n    return get_df_clean(\"ecuador/logistics/shrimp_processing_facilities/out/eu/out/ec_processing_facilities_eu.csv\",encoding=\"utf8\")\n\n@dlt.table(name=\"df1\")\ndef get_df1():\n    return get_df_clean(\"ecuador/logistics/shrimp_processing_facilities/acuaculturaypesca/3-cleaned/2_ACTUALIZA_PROCESA_PESCA_ACUA_22DICIEMBRE2017.csv\", encoding=\"utf8\")\n\n@dlt.table(name=\"df2\")\ndef get_df2():\n    return get_df_clean(\"ecuador/logistics/shrimp_processing_facilities/acuaculturaypesca/3-cleaned/2_tabula-ACTUALIZA_PROCESA_PESCA_ACUA_24OCTUBRE2017.csv\", encoding=\"utf8\")\n\n@dlt.table(name=\"df3\")\ndef get_df3():\n    return get_df_clean(\"ecuador/logistics/shrimp_processing_facilities/acuaculturaypesca/3-cleaned/2_ACTUALIZA_PROCESA_ACUACULTURA_12OCTUBRE2017.csv\", encoding=\"utf8\")\n\n@dlt.table(name=\"df1_2018\")\ndef get_df1_2018():\n    return get_df_clean(\"ecuador/logistics/shrimp_processing_facilities/acuaculturaypesca/3-cleaned/2_tabula-ACTUALIZA_PROCESA_PESCA_ACUA_13AGOSTO2018.csv\", encoding=\"utf8\")\n\n\n@dlt.table(name=\"df2_2018\")\ndef get_df2_2018():\n    return get_df_clean(\"ecuador/logistics/shrimp_processing_facilities/acuaculturaypesca/3-cleaned/2_tabula-ACTUALIZA_PROCESA_ACUACULTURA_31AGOSTO2018.csv\", encoding=\"utf8\")\n\n#zero_round()\n@dlt.table(name=\"first_round\")\ndef shrimp_man():\n    return shrimp_processing()\n\n\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b33c6c9d-50d9-4563-9213-75854f607dfe","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"shrimp_delta_poc_rewrite_submission","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2326965762994603}},"nbformat":4,"nbformat_minor":0}
