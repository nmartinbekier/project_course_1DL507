{"cells":[{"cell_type":"markdown","source":["# Data Science problems in supply chains with Trase.earth\n\nNicolás Martín and Oskar Åsbrink"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0409b42b-323c-4f0e-a169-290f76612e06","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Division of Work:\n- Graphs in Neo4j:\n  - Majority Nicolás\n- Delta Live Tables & Pipeline:\n  - roughly equally!\n- Amazon Web Services, Databricks Unity Catalog, Delta-sharing\n  - roughly equally!\n- Delta Lake documentation, Delta Live Table module for ScaDaMaLe\n  - Majority Oskar"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3037deaf-f0a1-4939-913c-f2528c452a2a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Neo4j\n## Sample json data\nThe following is a sample from a single record of Brazil's animal transportation registry (called GTA - Guia do Transporte Animal). 2 million of such records were imported into neo4j, which include records from 2013 - 2020 for the Brazilian state of Pará."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4375b83-f073-45f9-a80c-a3b76eb94c8b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["{\n    \"ID\": \"PA|K|102009\",\n    \"INFO_STATUS\": \"EM TRANSITO\",\n    \"ORIGIN_TAX_NUMBER\": \"56668880134\",\n    \"ORIGIN_NAME\": \"FAZ SAO MATEUS II\",\n    \"ORIGIN_FARMER\": \"CAIO GERONIMO DA SILVA\",\n    \"ORIGIN_CITY\": \"SAO FELIX DO XINGU\",\n    \"ORIGIN_STATE\": \"PA\",\n    \"DESTINATION_TAX_NUMBER\": \"05566585230\",\n    \"DESTINATION_NAME\": \"FAZENDA COLORADO DO RIO PARDO\",\n    \"DESTINATION_FARMER\": \"MATHEUS HENRIQUE BORGES SILVA\",\n    \"DESTINATION_CITY\": \"SAO FELIX DO XINGU\",\n    \"DESTINATION_STATE\": \"PA\",\n    \"TRANSPORT\": \"A PE\",\n    \"TIMELINE_OBS\": \"Nº DO CAR: PA-1507300-B7F0E10F2F0141E7B8AE65BA3E097743 GTA EMITIDA EM SUBSTITUICAO A GTA Nº 066640-K. EMISSOR ANTERIOR: JANDER MARINHO MACHADO (92222447291). ESCRITORIO: SAO FELIX DO XINGU\",\n    \"SPECIES_PURPOSE\": \"ENGORDA\",\n    \"GTA_SOURCE\": \"S1 2020\",\n    \"SPECIES_GROUP\": \"BOVIDEOS\",\n    \"SPECIES_NAME\": \"BOVINO\",\n    \"DESTINATION_CODE\": \"15073006602\",\n    \"DESTINATION_GEOCODE\": \"1507300\",\n    \"ORIGIN_CODE\": \"15073004972\",\n    \"ORIGIN_GEOCODE\": \"1507300\",\n    \"MISSING_INFO\": \"PRESENT\",\n    \"SPECIES\": \"BOV\",\n    \"TRANSPORT_DATE\": \"2020-01-07\",\n    \"TRANSPORT_YEAR\": 2020,\n    \"NUM_ANIMALS_ASSUMED\": false,\n    \"SLAUGHTER_GTA\": false,\n    \"ANIMALS\": [\n      {\n        \"AMOUNT_SENT\": 5,\n        \"DESCRIPTION\": \"BOVINO,MACHO,13 A 24 MESES\"\n      },\n      {\n        \"AMOUNT_SENT\": 80,\n        \"DESCRIPTION\": \"BOVINO,FEMEA,13 A 24 MESES\"\n      },\n      {\n        \"AMOUNT_SENT\": 15,\n        \"DESCRIPTION\": \"BOVINO,MACHO,ACIMA DE 36 MESES\"\n      },\n      {\n        \"AMOUNT_SENT\": 45,\n        \"DESCRIPTION\": \"BOVINO,FEMEA,ACIMA DE 36 MESES\"\n      }\n    ]\n  }"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3f6ac9f1-459b-44e5-a6fb-7a4173c9b95d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Creation of the graph\nBased on json files with the previous structure, we created the network graph with the following Cypher query:\n```\nCALL apoc.periodic.iterate(\"CALL apoc.load.json('file:///file_path/filename.json') YIELD value AS v\",\n\"MERGE (o_city:City {name:COALESCE(v.ORIGIN_CITY, '')})\nMERGE (d_city:City {name:COALESCE(v.DESTINATION_CITY, '')})\nMERGE (o_state:State {name:COALESCE(v.ORIGIN_STATE, '')})\nMERGE (d_state:State {name:COALESCE(v.DESTINATION_STATE, '')})\nMERGE (o_city)-[:IS_LOCATED_IN]->(o_state)\nMERGE (d_city)-[:IS_LOCATED_IN]->(d_state)\nMERGE (transp:Transport {transport:COALESCE(v.TRANSPORT, '')})\nMERGE (purp:Purpose {purpose:COALESCE(v.SPECIES_PURPOSE, '')})\nMERGE (gta_src:GTA_Source {gta_source:COALESCE(v.GTA_SOURCE, '')})\nMERGE (sp:Species {species:COALESCE(v.SPECIES, '')})\nMERGE (o_code:Code {code:COALESCE(v.ORIGIN_CODE, '')})\nMERGE (o_geocode:Geocode {geocode:COALESCE(v.ORIGIN_GEOCODE, '')})\nMERGE (d_code:Code {code:COALESCE(v.DESTINATION_CODE, '')})\nMERGE (d_geocode:Geocode {geocode:COALESCE(v.DESTINATION_GEOCODE, '')})\nMERGE (o_code)-[:HAS_GEOCODE]->(o_geocode)\nMERGE (d_code)-[:HAS_GEOCODE]->(d_geocode)\nMERGE (o_city)-[:HAS_GEOCODE]->(o_geocode)\nMERGE (d_city)-[:HAS_GEOCODE]->(d_geocode)\nMERGE (gta:GTA {gta_id:COALESCE(v.ID, '')}) \n  ON CREATE SET gta.info_status = v.INFO_STATUS,\n                gta.timeline_obs = v.TIMELINE_OBS,\n                gta.missing_info = v.MISSING_INFO,\n                gta.transport_date = v.TRANSPORT_DATE,\n                gta.transport_year = v.TRANSPORT_YEAR,\n                gta.num_animals_assumed = v.NUM_ANIMALS_ASSUMED,\n                gta.slaughter_gta = v.SLAUGHTER_GTA,\n                gta.total_animals = 0\nFOREACH (an_sent IN v.ANIMALS | \n    MERGE (an_type:Animal_Type {animal_type:COALESCE(an_sent.DESCRIPTION, '')})\n    CREATE (gta)-[:ANIMAL_TYPE_SENT {amount:COALESCE(an_sent.AMOUNT_SENT, '')}]->(an_type)\n    SET gta.total_animals = gta.total_animals + an_sent.AMOUNT_SENT\n)\nCREATE (gta)-[:IS_TRANSPORTED_BY]->(transp)\nCREATE (gta)-[:HAS_PURPOSE]->(purp)\nCREATE (gta)-[:FROM_GTA_SOURCE]->(gta_src)\nCREATE (gta)-[:SPECIES_TRANSPORTED]->(sp)\nMERGE (o_farm:Farm {tax_number:COALESCE(v.ORIGIN_TAX_NUMBER, ''), name:COALESCE(v.ORIGIN_NAME, '')}) \nMERGE (o_farm)-[:HAS_CODE]->(o_code)\nCREATE (o_farm)-[:IS_ORIGIN_OF_GTA]->(gta)\nMERGE (o_farm)-[:IS_LOCATED_IN]->(o_city)\nFOREACH (i IN CASE WHEN NOT gta.slaughter_gta THEN [1] ELSE [] END | \n    MERGE (d_farm:Farm {tax_number:COALESCE(v.DESTINATION_TAX_NUMBER,''), name:COALESCE(v.DESTINATION_NAME,'')}) \n    MERGE (d_farm)-[:HAS_CODE]->(d_code)\n    MERGE (o_farm)-[sends:SENDS_TO]->(d_farm)\n        ON CREATE SET sends.total_sent = gta.total_animals\n        ON MATCH SET sends.total_sent = sends.total_sent + gta.total_animals\n    MERGE (o_farm)-[y_stats:YEAR_SENDS_STATS {year:gta.transport_year}]->(d_farm)\n        ON CREATE SET y_stats.total_sent = gta.total_animals\n        ON MATCH SET y_stats.total_sent = y_stats.total_sent + gta.total_animals        \n    MERGE (gta)-[:GTA_HAS_DESTINATION]->(d_farm)\n    MERGE (d_farm)-[:IS_LOCATED_IN]->(d_city)\n)\nFOREACH( i IN CASE WHEN gta.slaughter_gta THEN [1] ELSE [] END | \n    MERGE (slaughter:Slaughterhouse {tax_number:COALESCE(v.DESTINATION_TAX_NUMBER,''), name:COALESCE(v.DESTINATION_NAME,'')}) \n    MERGE (slaughter)-[:HAS_CODE]->(d_code)\n    MERGE (o_farm)-[sends:SENDS_TO]->(slaughter)\n        ON CREATE SET sends.total_sent = gta.total_animals\n        ON MATCH SET sends.total_sent = sends.total_sent + gta.total_animals\n    MERGE (o_farm)-[y_stats:YEAR_SENDS_STATS {year:gta.transport_year}]->(slaughter)\n        ON CREATE SET y_stats.total_sent = gta.total_animals\n        ON MATCH SET y_stats.total_sent = y_stats.total_sent + gta.total_animals          \n    MERGE (gta)-[:GTA_HAS_DESTINATION]->(slaughter)\n    MERGE (slaughter)-[:IS_LOCATED_IN]->(d_city)\n)\n\", {})\n\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5adab865-d4ba-4adc-943e-f88b5122fa71","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Relevant graph queries\n1. Structure of the graph. Generated using `call apoc.meta.graph()`\n![Graph structure](https://github.com/oskarasbrink/ECLAT-DataMining-Project/blob/main/temp_images/2022_12_15_graph_structure.png?raw=true)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ebd4a6c4-a985-471e-b6c6-726193719844","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["2. Example of a simple relationship between farms, slaughterhouses, GTA and City. Generated using `MATCH p=(n:Farm)--(g:GTA)--(s:Slaughterhouse)--(c:City) WHERE s.name<>'' RETURN n,g,s,c LIMIT 5`\n![Relationships example](https://github.com/oskarasbrink/ECLAT-DataMining-Project/blob/main/temp_images/2022_12_15_Graph_example_of_relationship.png?raw=true)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9b9e59d1-c688-4696-b765-c5ee7481b010","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["3. Top aggregated exchanges from farms to other farms or slaughterhouses. Generated using \n```\nMATCH (n:Farm)-[s:SENDS_TO]->(m) RETURN n.name AS Origin_farm, m.name AS Destination_farm, \ns.total_sent AS Sent_animals, labels(m) AS Destination_type ORDER BY s.total_sent DESC LIMIT 100\n```\n![Relationships example](https://github.com/oskarasbrink/ECLAT-DataMining-Project/blob/main/temp_images/2022_12_15_Top_aggregated_exchanges.png?raw=true)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91e3779d-2c22-4c64-a152-8179e52c927c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["3. Example of farms which are indirect suppliers (at 2 degrees of separation) to a Slaughterhouse. Generated using\n```MATCH p=(s:Slaughterhouse)<-[st:SENDS_TO*2..2]-(f:Farm) RETURN p LIMIT 20``` \n![Indirect suppliers example](https://github.com/oskarasbrink/ECLAT-DataMining-Project/blob/main/temp_images/2022_12_15_Indirect_suppliers_example.png?raw=true)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb197773-bb09-4f9d-be1e-49fc0b2a1763","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Databricks Unity Catalog and Delta Sharing"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1955f443-dcce-4898-8481-0f0bcf41a94a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Setting up AWS IAM roles and policies for Unity Catalog and S3 buckets\n\nCustom trust policy role created, with a trust relationship to Databricks Unity Catalog service\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"<databricks_account_id>\"\n                }\n            }\n        }\n    ]\n}\n```\n\nPolicy for letting Unity Catalog use the S3 bucket for the metastore linked with Unity Catalog\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:GetLifecycleConfiguration\",\n                \"s3:PutLifecycleConfiguration\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::<bucket_name_for_metastore>/*\",\n                \"arn:aws:s3:::<bucket_name_for_metastore>\",\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Action\": [\n                \"sts:AssumeRole\"\n            ],\n            \"Resource\": [\n                \"arn:aws:iam::<aws id>:role/<name of role created above>\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ada63368-9ada-4431-9479-6c3ed25f6b37","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Create a Delta Table from a csv on an S3\nThe S3 object is a 10GB csv with relevant look-up information, which will be loaded into a Delta table within Unity Catalog. It will later be made externally available to access or query as a Delta Share [for external users to load, query, or connect to a Business Intelligence tool. ]."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2bc6e2e-f660-487e-a70a-6ac37e44342d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Setting schema automerge to have databricks resolve schema differences when merging\nspark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")\n\n## Reading the csv into a spark dataframe\n# File location and type\nfile_location = \"s3a://path/file.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \";\"\n\n# Create the spark dataframe\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\n# Create a spark table called 'df_table' based on the dataframe\ndf.createOrReplaceTempView('df_table')\n\n# Create an empty Delta table based on the schema of 'df_table'\nspark.sql(\"CREATE TABLE IF NOT EXISTS <catalog>.<db>.<table_name> \\\n    USING DELTA \\\n    PARTITIONED BY (<col_name>) \\\n    SELECT * from df_table \\\n    WHERE 1=2\")\n\n# Write to the Delta Table created\ndf.write.format(\"delta\").mode(\"append\").insertInto(\"<catalog>.<db>.<table_name>\")\n\n# Show some records to review it has been loaded properly\ndisplay(spark.table(\"<catalog>.<db>.<table_name>\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3f7447a7-c6c1-4696-989f-209d0d9e670f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Screenshots of the results of running the previous scripts on a 10GB CSV file on a spark cluster made of 1 driver and 2 workers, each with 4 cores and 8GB memory. \n![Reading the csvs](https://github.com/oskarasbrink/ECLAT-DataMining-Project/blob/main/temp_images/2022_12_15_Reading_the_csv.png?raw=true)\n![Creating an empty table with the csv schema](https://github.com/oskarasbrink/ECLAT-DataMining-Project/blob/main/temp_images/2022_12_15_Create_delta_table.png?raw=true)\n![Display Unity Catalog table](https://github.com/oskarasbrink/ECLAT-DataMining-Project/blob/main/temp_images/2022_12_15_Display_table.png?raw=true)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b3292320-1471-4a2f-97ae-fce47e202ffe","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Reading a shared Delta Table from an external (non-Databricks) location"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e73e8339-4f4d-4869-94e9-cbfedb4cfd0b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Install the delta sharing library\n%pip install delta-sharing\nimport delta_sharing\n\n# Open a client using credentials (shared by the provider of the data)\nprofile_path = f\"/path_to_share_credentials/config.share\"\nclient = delta_sharing.SharingClient(profile_path)\n\n# See all tables being shared\nclient.list_all_tables()\n\nshare_name = f\"shared_resource\"\nschema_name = f\"brazil\"\n# Load a table as a pandas dataframe\n# .. also possible through spark, hive, or connectors from 3rd parties (Power BI, Tableu, etc..)\ncandy_df = delta_sharing.load_as_pandas(f\"{profile_path}#{share_name}.{schema_name}.{table_name}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c62acf42-7074-4064-8351-eb0d70a5afc9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Screenshot of reading an example share called 'candyland' from a Jupyter Notebook in SNIC cloud, through pandas\n![Delta Sharing example using](files/tables/trase/2022_12_16_Delta_sharing.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8740257a-dfa7-4ea0-93e1-3bc9d8ea49ac","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Delta Live Tables\n\n__See below of example changes made to scripts, and see `shrimp_delta_poc_rewrite_submission.ipynb` and `ec_shrimp_area_production_per_parish_rewrite.ipynb` for examples on how a script was transformed to be able to run in a Pipeline.__"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"79e06dd7-313c-4761-b95f-782808691cec","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Example of changes to Trase data preprocessing-scripts\n\nThe main task for this part of the project is to implement and use Delta Live Tables (DLT) to help with the preprocessing pipeline. To make this easy for Trase, the main approach was to rewrite their load and save functions, and mount an existing S3 bucket to Databricks, to integrate DLT code instead of AWS-cli code."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25d21097-7ca1-4706-b98e-fa869f963326","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["### Original function, without modifications ###\n\ndef get_df(\n    key,\n    sep=\";\",\n    bucket_name=\"trase-uppsala\",\n    type=None,\n    skiprows=None,\n    encoding=\"latin1\",\n):\n    data = read_s3_csv(key, bucket_name=bucket_name)\n    if skiprows:\n        return remove_whitespace(\n            pd.read_csv(\n                io.BytesIO(data),\n                sep=sep,\n                encoding=encoding,\n                dtype=type,\n                skiprows=skiprows,\n            )\n        )\n    return remove_whitespace(\n        pd.read_csv(io.BytesIO(data), sep=sep, encoding=encoding, dtype=type)\n    )\n\ndef remove_whitespace(df):\n    for col in df.columns:\n        if df[col].dtype in (np.object, np.str):\n            df[col] = df[col].str.strip()\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dd74288d-d9dd-4d04-a55c-dde064a4863a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["### Modified function, loading existing Delta Table instead of fetching from S3 ###\ndef get_df(table_name):\n    return ps.DataFrame(dlt.read(table_name)).to_pandas()\n\n\n### used when creating delta tables ###\ndef get_df_clean(key,sep=\";\",encoding=\"latin1\",bucket_name=\"s3a://uutrase/data/trase-uppsala/\",type=None,skiprows=None):\n    \n    return spark.createDataFrame(remove_whitespace(spark.read.option(\"delimiter\", sep)\n                             .option(\"encoding\",\"UTF-8\")\n                             .option(\"header\", \"true\")\n                             .option(\"multiline\",\"true\")\n                             .csv(bucket_name + key).toPandas()))\ndef remove_whitespace(df):\n    for col in df.columns:\n        if df[col].dtype in (np.object, np.str):\n            df[col] = df[col].str.strip()\n            \n    return df\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69a05875-7895-4a3b-a4dc-2e0f35d95b27","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# creating a delta live table\n@dlt.table(name=\"city_df\")\ndef get_city_df():\n    return get_df_clean(\"s3://bucket/path_to_data/data.csv\", sep=\",\", encoding=\"utf8\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0804eba5-9fca-4a90-84d6-672190d8f586","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Resulting pipeline in Databricks\n<img src=\"https://github.com/oskarasbrink/ECLAT-DataMining-Project/blob/main/temp_images/pipeline.png?raw=true\" width=\"500\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d129b9ef-89f6-4299-bc51-06b4667ca602","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Main_code_submission_notebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2326965762994595}},"nbformat":4,"nbformat_minor":0}
